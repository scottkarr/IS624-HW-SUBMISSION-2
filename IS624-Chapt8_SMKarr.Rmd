---
title: "Regression Trees and Rule-Based Models"
---

https://github.com/cran/AppliedPredictiveModeling/blob/master/inst/chapters/08_Regression_Trees.R
https://www.guru99.com/r-decision-trees.html


##### Chapter 8 KJ 8.1, 8.2, 8.3, 8.7
```{r, ch08_init, eval=TRUE, echo=FALSE}
### Load packages
suppressMessages(library("AppliedPredictiveModeling"))
suppressMessages(library("caret"))
suppressMessages(library("ipred"))
suppressMessages(library("mlbench"))
suppressMessages(library("party"))
suppressMessages(library("randomForest"))
suppressMessages(library("gbm"))
suppressMessages(library("rpart"))
suppressMessages(library("Cubist"))
suppressMessages(library("dplyr"))
suppressMessages(library("gridExtra"))
```

### Problem 1 
Recreate the simulated data from Exercise 7.2:

Per 7.2, Friedman (1991) introduced several benchmark data sets create by simulation. One of these simulations used the following nonlinear equation to create data:

$$y = 10 sin(πx1x2) + 20(x3 − 0.5)2 + 10x4 + 5x5 + N(0, σ2)$$
where the x values are random variables uniformly distributed between [0, 1] (there are also 5 other non-informative variables also created in the simulation). 
```{r, ch08_a, eval=TRUE, echo=TRUE, results="hide"}
set.seed(200)
simulated <- mlbench.friedman1(200, sd = 1)
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated)
colnames(simulated)[ncol(simulated)] <- "y"
```

#### (a) Fit a random forest model to all of the predictors, then estimate the variable importance scores:
```{r, ch08_a.1, eval=TRUE, echo=TRUE}
model1 = randomForest( y ~ ., data=simulated, importance=TRUE, ntree=1000 )
rfImp1 = varImp(model1, scale=FALSE)
rfImp1 = rfImp1[ order(-rfImp1), , drop=FALSE ]
print("randomForest (no correlated predictor)")
print("Table 1: Variable importance scores for part (a) simulation.")
print(rfImp1)
```
Q.  Did the random forest model significantly use the uninformative predictors (V6 – V10)?
A.  The predictor significance for the simulated data set in this model can be seen in 
    Table 1. The model weights predictors V1,V4,V2,V5,V3 in order of diminishing significance
    and trailing off after that.

##### (b) Fit another random forest model to these data. Did the importance score for V1 change? What happens when you add another predictors that is also highly correlated with V1?
```{r, ch08_b.1, eval=TRUE}
simulated$duplicate1 <- simulated$V1 + rnorm(200) * .1
cor(simulated$duplicate1, simulated$V1)
```

After adding a highly correlated, predictors are ordered V4,V1,V2,duplicate1,V5 in order of diminishing significance and trailing off after that.  V1 moved down to 2nd place in ranking.
```{r, ch08_b.2, eval=TRUE}
model2 = randomForest( y ~ ., data=simulated, importance=TRUE, ntree=1000 )
rfImp2 = varImp(model2, scale=FALSE)
rfImp2 = rfImp2[ order(-rfImp2), , drop=FALSE ] 
print("randomForest (one correlated predictor)")
print("Table 2: Variable importance scores for part (b) simulation.")
print(rfImp2)
```

```{r, ch08_b.3, eval=TRUE}
simulated$duplicate2 = simulated$V1 + rnorm(200) * .1
cor(simulated$duplicate2,simulated$V1)
```

Adding a 2nd highly correlated variable, predictors are ordered V2,V2,V1,duplicate2,duplicate1 moving V1 to 3rd rank.
```{r, ch08_b.4, eval=TRUE}
model3 = randomForest( y ~ ., data=simulated, importance=TRUE, ntree=1000 )
rfImp3 = varImp(model3, scale=FALSE)
rfImp3 = rfImp3[ order(-rfImp3), , drop=FALSE ] 
print("randomForest (two correlated predictors)")
print(rfImp3)
```

##### c) Study this when fitting conditional inference trees:

Use the cforest function in the party package to fit a random forest model using conditional inference trees. The party package function varimp can calculate predictor importance. The conditional argument of that function toggles between the traditional importance measure and the modified version described in Strobl et al. (2007). 

Do these importances show the same pattern as the traditional random forest model?
Yes, the conditional inference model has a similar pattern of importance as the random forest model from Part (a).  Predictor's rank importance scores for the conditional inference random forests are shown as follows . . .

no correlated predictor       V1,V4,V2,V5,V7
one correlated predictor and  V4,V1,V2,duplicate1,V5
two correlated predictor      V4,V1,V2,duplicate2,duplicate1

So once again, adding highly correlated predictors reduces the value other predictors and reduces the rank of V1.
```{r, ch08_c.1, eval=TRUE}
simulated$duplicate1 = NULL
simulated$duplicate2 = NULL

model1 = cforest( y ~ ., data=simulated )
cfImp1 = as.data.frame(varimp(model1),conditional=TRUE)
cfImp1 = cfImp1[ order(-cfImp1), , drop=FALSE ] 
print(sprintf("cforest (no correlated predictor);varimp(*,conditional=%s)",TRUE))
print(cfImp1)

# Now we add correlated predictors one at a time 
simulated$duplicate1 = simulated$V1 + rnorm(200) * 0.1

model2 = cforest( y ~ ., data=simulated )
cfImp2 = as.data.frame(varimp(model2),conditional=use_conditional_true)
cfImp2 = cfImp2[ order(-cfImp2), , drop=FALSE ]  
print(sprintf("cforest (one correlated predictor);varimp(*,conditional=%s)",TRUE))
print(cfImp2)

simulated$duplicate2 = simulated$V1 + rnorm(200) * 0.1

model3 = cforest( y ~ ., data=simulated )
cfImp3 = as.data.frame(varimp(model3),conditional=TRUE)
cfImp3 = cfImp3[ order(-cfImp3), , drop=FALSE ] 
print(sprintf("cforest (two correlated predictor); varimp(*,conditional=%s)",TRUE))
print(cfImp3)
```

#### (d) Repeat this process with different tree models, such as boosted trees and Cubist. 
Does the same pattern occur?

The gbm pattern does re-rank the importance of predictors but with less change in rank.  Adding an extra highly correlated predictor with has a lesser impact on the overall importance of predictors compared to that of random forest.

no correlated predictor       V4,V1,V2,V5,V3
one correlated predictor and  V4,V2,V1,duplicate1,V5
two correlated predictor      V4,V2,V1,V5,V3

```{r, ch08_d.1, eval=TRUE}
simulated$duplicate1 = NULL
simulated$duplicate2 = NULL
    
model1 = gbm( y ~ ., data=simulated, distribution="gaussian", n.trees=1000 ) 
print(sprintf("gbm (no correlated predictor)"))
print(summary(model1,plotit=F)) # the summary method gives variable importance ... 

# Now we add correlated predictors one at a time 
simulated$duplicate1 = simulated$V1 + rnorm(200) * 0.1

model2 = gbm( y ~ ., data=simulated, distribution="gaussian", n.trees=1000 ) 
print(sprintf("gbm (one correlated predictor)"))
print(summary(model2,plotit=F))

simulated$duplicate2 = simulated$V1 + rnorm(200) * 0.1

model3 = gbm( y ~ ., data=simulated, distribution="gaussian", n.trees=1000 ) 
print(sprintf("gbm (two correlated predictor)"))
print(summary(model3,plotit=F))

set.seed(200)
simulated1 <- mlbench.friedman1(200, sd = 1)
simulated1 <- cbind(simulated1$x, simulated1$y)
simulated1 <- as.data.frame(simulated1)
colnames(simulated1)[ncol(simulated1)] <- "y"

set.seed(200)
simulated2 <- 
  simulated1 %>% 
  mutate(duplicate1 = V1 + rnorm(200) * .1)

cor(simulated2$duplicate1, simulated2$V1)

# add another correlated variable
set.seed(5)
simulated3 <- 
  simulated2 %>% 
  mutate(duplicate2 = V1 + rnorm(200) * .1)

cor(simulated3$duplicate2, simulated3$V1)

gbm1 <- train(y ~ ., data = simulated1, method = "gbm", verbose = F)
gbm2 <- train(y ~ ., data = simulated2, method = "gbm", verbose = F)
gbm3 <- train(y ~ ., data = simulated3, method = "gbm", verbose = F)


gridExtra::grid.arrange(
  plot(varImp(gbm1, scale = F), main = "No correlation"),
  plot(varImp(gbm2, scale = F), main = "2 correlated variables"),
  plot(varImp(gbm3, scale = F), main = "3 correlated variables"),
  ncol = 3
)
```

For Cubist indicates that predictors V1–V5 are at the top of the importance ranking. Adding an extra highly correlated predictor with V1 has very little impact on the importance scores when using Cubist.
```{r, ch08_d.2, eval=TRUE}
vnames <- c('V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10')
cbFit1 <- cubist(x = simulated[, 1:10],
                 y = simulated$y,
                 committees = 100)
cbImp1 <- varImp(cbFit1)
names(cbImp1) <- "Original"
cbImp1$Variable <- factor(rownames(cbImp1), levels = vnames)

cbFit2 <- cubist(x = simulated[, names(simulated) != "y"],
                 y = simulated$y, committees = 100)
cbImp2 <- varImp(cbFit2)
names(cbImp2) <- "Extra"

cbImp2$Variable <- factor(rownames(cbImp2), levels = vnames)
cbImp <- merge(cbImp1, cbImp2, all = TRUE) 
#rownames(cbImp) <- cbImp$Variable #this won't knit, commenting out
#cbImp$Variable <- NULL
print(cbImp)
```

### Problem 2 

#### a) Use a simulation to show tree bias with different granularities.

From 8.2, . . . intuitively, predictors that appear higher in the tree (i.e., earlier splits) or those that appear multiple times in the tree will be more important than predictors that occur lower in the tree or not at all. even if the predictor has little-to-no relationship with the response. 

This simulation uses one categorical predictor splitting the response into two groups. As a comparison, a similar simulation uses a continuous predictor that doesn't split the response into two  groups. 

simulations where X1 is categorical and X2 is continuous
```{r, ch08_2.1, eval=TRUE}
X_categorical <- rep(1:2,each=100)
Y <- X_categorical + rnorm(200,mean=0,sd=4)
set.seed(103)
X_continuous <- rnorm(200,mean=0,sd=2)
simData <- data.frame(Y=Y,X_categorical=X_categorical,X_continuous=X_continuous)
```

Note predictor X1 splits the response into two response groups and predictor X2, is independent of response. 

#### b) Plot frequency of predictor selection for tree bias simulation. 

|-----------------|
|X_categorical 52 |  
|X_continuous 44  |
|-----------------|

```{r, ch08_2.2, eval=TRUE}
boxplot(
  Y~X_categorical,
  #data=Y, 
  main="Tree Bias Simulation with categorical predictior",
  xlab="X_categorical", 
  ylab="Y"
)

plot(
  X_continuous, 
  Y, 
  main="Tree Bias Simulation with continous predictor",
  xlab="X_continuous", 
  ylab="Y", 
  pch=19
)
```

### Problem 3
In stochastic gradient boosting the bagging fraction and learning rate will govern the construction of the trees as they are guided by the gradient. Although the optimal values of these parameters should be obtained through the tuning process, it is helpful to understand how the magnitudes of these parameters affect magnitudes of variable importance. 

Figure 2 provides the variable importance plots for boosting using two extreme values for the bagging fraction (0.1 and 0.9) and the learning rate (0.1 and 0.9) for the solubility data. The left-hand plot has both parameters set to 0.1, and the right-hand plot has both set to 0.9.
```{r, ch08_3.a, eval=TRUE, echo=FALSE}
# library(AppliedPredictiveModeling)
# data(solubility)
# 
# X <- solTrainXtrans; Y <- solTrainY
# indx <- createFolds(Y, returnTrain = T)
# ctrl <- trainControl(method = "cv", index = indx)
# 
# # get tuning parameters for depth & number of trees for solubility dataset
# set.seed(624)
# tg0 <- expand.grid(interaction.depth=seq(1,7,by=2), n.trees=seq(100,1000,by=100), shrinkage = c(0.01, 0.1), n.minobsinnode=10)
# m0 <- train(X, Y, method="gbm", tuneGrid=tg0, trControl=ctrl, verbose=F)
# int_dpth <- m0$bestTune$interaction.depth
# ntrees <- m0$bestTune$n.trees
# 
# # apply to 
# set.seed(624)
# tg1 <- expand.grid(interaction.depth=int_dpth,n.trees=ntrees, shrinkage=0.1, n.minobsinnode=10)
# m1 <- train(X, Y, method = "gbm", tuneGrid=tg1,trControl=ctrl, bag.fraction=0.1, verbose=F)
# 
# set.seed(624)
# tg2 <- expand.grid(interaction.depth=int_dpth,n.trees=ntrees, shrinkage=0.9, n.minobsinnode=10)
# m2 <- train(X, Y, method = "gbm", tuneGrid=tg2,trControl=ctrl, bag.fraction=0.9, verbose=F)
# 
# gridExtra::grid.arrange(
#   plot(varImp(m1, scale=F), top=25, scales=list(y=list(cex=.95)), main = "bagging model 1"),
#   plot(varImp(m2, scale=F), top=25, scales=list(y=list(cex=.95)), main = "bagging model 2"),
#   ncol = 2
# )
```
![fig Bagging Models](/Users/scottkarr/IS624Fall2019/HW8/fig8.3.png)

#### (a) Why does the model on the right focus its importance on just the first few of predictors,whereas the ####     model on the left spreads importance across more predictors?
    
          Using the train() function to fit 2 models with parameters
          
           interaction depths (interaction.depth)
           fitted trees (n.trees)
           learning rates (shrinkage)
           minimum of 10 observations in the trees terminal nodes (n.minobsinnode) 
            
          Comparing their parameters:
              
          m1 with a 0.1 learning rate and 0.1 training observations (bag.fraction) against
          m2 uses a 0.9 learning rate and 0.9 training observations (bag.fraction).
          
          A weaker learning rate and lower bagging fraction reduces the number of predictors reduces    
          randomness in the model.
    
#### (b) Which model do you think would be more predictive of other samples?

          The model on the left is likely to have better performance due to fewer predictors and less 
          randomness.  Lower learning rate that is more random should be more predictive of other samples. 

          Higher bagging fraction is associated with greater model randomness thus increasing predictivity of 
          model.
    
    ```{r, ch08_3.b, eval=TRUE, echo=FALSE}
    #m.1LearnRate<-min(m2$results$RMSE)
    m.1LearnRate <- 0.7703595
    m.1LearnRate
    #m.9LearnRate<-min(m3$results$RMSE)
    m.9LearnRate <- 0.785859
    m.9LearnRate
    ```

#### (c) How would increasing interaction depth affect the slope of predictor importance for either model in Figure 2?

    As Tree depth increases variable importances disperse over more predictors thus flattening the slope.

    # ```{r, ch08_3.c, eval=TRUE, echo=FALSE}
    # plot(tune0)
    # ```
![Tree Depth](/Users/scottkarr/IS624Fall2019/HW8/TreeDepth.png)

### Problem 7
8.7. Refer to Exercises 6.3 and 7.5 which describe a chemical manufacturing process. Use the same data imputation, data splitting, and pre-processing steps as before and train several tree-based models:

(a) Which tree-based regression model gives the optimal resampling and test set performance?

(b) Which predictors are most important in the optimal tree-based regression model? Do either the biological or process variables dominate the list? How do  
    the top 10 important predictors compare to the top 10 predictors from the optimal linear and nonlinear models?

(c) Plot the optimal single tree with the distribution of yield in the terminal nodes. Does this view of the data provide additional knowledge 
    about the biological or process predictors and their relationship with yield?

#### (a) We set aside 20% of the observations to be the test dataset.
```{r, ch08_7, eval=TRUE, echo=FALSE}
data(ChemicalManufacturingProcess) # mlbench
set.seed(5)
# preprocess for trees, impute missing
chem_preprocess <- preProcess(ChemicalManufacturingProcess, method = c("bagImpute"))
chem_df <- predict(chem_preprocess, ChemicalManufacturingProcess)

# train-test partition
training_rows <- createDataPartition(chem_df$Yield, p =.8, list = F)
x_train <- chem_df[training_rows, ]
x_test <- chem_df[-training_rows, ]
y_test <- chem_df[-training_rows, "Yield"]
# parms for all models . . .
ctrl <- trainControl(method="cv", number=5, allowParallel=T, savePredictions="final")
```

```{r, ch08_7a, eval=TRUE, echo=FALSE}
#### CART Model ####
set.seed(5)
mcart <- train(Yield ~., data = x_train, method='rpart',metric="RMSE", trControl=ctrl, tuneLength=10)
(mcart$bestTune)
data.frame(model="CART", mcart$bestTune, RMSE=min(mcart$results$RMSE), row.names="")
plot(mcart)
predict(mcart, x_test)
```

```{r, ch08_7b, eval=TRUE, echo=FALSE}
#### Random Forest Model ####
set.seed(5)
rf_grid <- expand.grid(mtry = seq(5, 30, 5))
mrf <- train(Yield ~ ., data = x_train,method="rf",metric="RMSE",trControl=ctrl,tuneGrid=rf_grid, ntree=1000, importance=T)

(mrf$bestTune)
data.frame(model="Random Fores", mrf$bestTune, RMSE=min(mrf$results$RMSE), row.names="")
plot(mrf)
predict(mrf, x_test)
```

```{r, ch08_7c, eval=TRUE, echo=FALSE}
#### Extreme Gradient Boosting Trees ####
set.seed(5)
xgb_grid <-  expand.grid(nrounds=250,
                        eta = c(0.025, .05, .1), #.05, # 
                        max_depth = c(5, 10, 20), #5, #
                        colsample_bytree = seq(.25, 1, .25), #.25, #
                        gamma = 0,
                        min_child_weight = 0,
                        subsample = 1
                        )

mxgb <- train(Yield ~ ., data=x_train, method="xgbTree", trControl=ctrl, tuneGrid=xgb_grid)
(mxgb$bestTune)
data.frame(model="Gradient Boosting", mxgb$bestTune, RMSE=min(mxgb$results$RMSE), row.names="")
plot(mxgb)
predict(mxgb, x_test)
```

```{r, ch08_7p, eval=TRUE, echo=FALSE}
# compare models on RMSE
df_mcarts <- data.frame(Model="carts",RMSE=mcart$results$RMSE, Rsquared=mcart$results$Rsquared)
df_mcarts <- df_mcarts[df_mcarts$RMSE == min(df_mcarts$RMSE),]
df_results <- df_mcarts
df_mrf <- data.frame(Model="rf",RMSE=mrf$results$RMSE, Rsquared=mrf$results$Rsquared)
df_mrf<- df_mrf[df_mrf$RMSE == min(df_mrf$RMSE),]
df_results <- rbind(df_results, df_mrf)
df_mxgb <- data.frame(Model="xgboost",RMSE=mxgb$results$RMSE, Rsquared=mxgb$results$Rsquared)
df_mxgb <- df_mxgb[df_mxgb$RMSE == min(df_mxgb$RMSE),]
df_results <- rbind(df_results, df_mxgb)

ggplot() + geom_col(aes(x = reorder(df_results$Model, -df_results$RMSE), y = df_results$RMSE)) + xlab("MODEL") + ylab("RMSE")
ggplot() + geom_col(aes(x = reorder(df_results$Model, -df_results$Rsquared), y = df_results$Rsquared)) + xlab("MODEL") + ylab("Rsquared")
```

#### (a) Which tree-based regression model gives the optimal resampling and test set performance?
The test data used for predictions for CARTS, Random Forest and Gradient Boosting had RMSE values of 1.59, 1.23 and 1.14 respectively.
The gradient boosting model with 10 tree-depth and .100 eta achieved this fit and appears to be the optimal model of those attempted.

```{r, ch08_7i, eval=TRUE, echo=FALSE}
dotPlot(varImp(mcart), top=15)
dotPlot(varImp(mrf), top=15)
dotPlot(varImp(mxgb), top=15)
```

#### (b) Which predictors are most important in the optimal tree-based regression model? Do either the biological or process variables dominate the list? How do the top 10 important predictors compare to the top 10 predictors from the optimal linear and nonlinear models?

The most important predictors in the gradient boosting model are . . .
    
    1. ManufacturingProcess32
    2. ManufacturingProcess31
    3. BiologicalMaterial12
    4. ManufacturingProcess13
    5. ManufacturingProcess09
    
Manufacturing process dominate the top of the list.  The other two models have a more balanced mixture fo biological and process predictors although in both gradient boosting and random forest, ManufacturingProcess32 plays on outsized role-by more than a factor of 2.  Also, the
CART model shows a steep drop-off in variable importance after the 10th variable.  Other models diminish more gradually with the exception of
ManfuacturingProcess32--the top entry.


```{r, ch08_7j, eval=TRUE, echo=FALSE}
gridExtra::grid.arrange(
  plot(varImp(mrf), main = "Random Forest"),
  plot(varImp(mcart), main = "CART"),
  plot(varImp(mxgb), main = "Gradient Boosting"),
  ncol = 3
)
```

#### (c) Plot the optimal single tree with the distribution of Yield in the terminal nodes. Does this view of the data provide additional knowledge about the biological or process predictors and their relationship with Yield? 
There's no new information about predictor names, but more detail about how they are evaluated  The tree view shows the principal split point and percentages for each side of the split but does not provide any additional information about the predictors. The model yield is 40 for ManufacturingProcess32 the top level split.  This node evaluated for > 160, and splits into yes > BiologicalMaterial11 < 145 58% nad no 42% > ManufacturingProcess09 < 45.  These are further subdivided.  

```{r, ch08_7k, eval=TRUE, echo=FALSE}
library(rpart.plot)
rpart.plot(mcart$finalModel)
```